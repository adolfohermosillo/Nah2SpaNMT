{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nahuatl-Spanish Neural Machine Translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Español</th>\n",
       "      <th>Náhuatl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>De Porfirio Diaz a Zapata</td>\n",
       "      <td>De Porfirio Diaz a Zapata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Documentos nauas de la Ciudad de México del si...</td>\n",
       "      <td>Documentos nauas de la Ciudad de México del si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Documentos nauas de la Ciudad de México del si...</td>\n",
       "      <td>Documentos nauas de la Ciudad de México del si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Documentos nauas de la Ciudad de México del si...</td>\n",
       "      <td>Documentos nauas de la Ciudad de México del si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Documentos nauas de la Ciudad de México del si...</td>\n",
       "      <td>Documentos nauas de la Ciudad de México del si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Español  \\\n",
       "0                          De Porfirio Diaz a Zapata   \n",
       "1  Documentos nauas de la Ciudad de México del si...   \n",
       "2  Documentos nauas de la Ciudad de México del si...   \n",
       "3  Documentos nauas de la Ciudad de México del si...   \n",
       "4  Documentos nauas de la Ciudad de México del si...   \n",
       "\n",
       "                                             Náhuatl  \n",
       "0                          De Porfirio Diaz a Zapata  \n",
       "1  Documentos nauas de la Ciudad de México del si...  \n",
       "2  Documentos nauas de la Ciudad de México del si...  \n",
       "3  Documentos nauas de la Ciudad de México del si...  \n",
       "4  Documentos nauas de la Ciudad de México del si...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pd.read_csv('axolotl.csv')\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data\n",
    "A function that given a list of strings returns only alphanumeric symbols in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case sentences and return only letters and numbers. \n",
    "def clean_dataset(data, numeric = True):\n",
    "    if numeric:\n",
    "        return [\"\".join([i for i in sentence if i.isalnum() or i == ' ']).lower() for sentence in data]\n",
    "    else:\n",
    "        return [\"\".join([i for i in sentence if i.isalpha() or i == ' ']).lower() for sentence in data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that given a list of strings returns only alphanumeric symbols and some punctuations marks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def further_clean_dataset(data):\n",
    "    is_mark = lambda i: i in ['.', ',', \"'\", \"'\", '?', '!', '¿', '(', ')','¡','-'] \n",
    "    mark_sapce = lambda i: \" \"+i+\" \" if is_mark(i) else i\n",
    "    #alphanumeric or space\n",
    "    anumsap = lambda i: i.isalnum() or i == ' '\n",
    "    return [\"\".join([mark_sapce(i) for i in sentence if anumsap(i) or is_mark(i)]).lower() for sentence in data]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing clean Data\n",
    "A function to write cleaned data into text files in a format that will be used by Deep learning algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(source_data, target_data, tokenization, set_type, path = '../'):\n",
    "    \n",
    "    src = open(path+\"/\"+tokenization+ '-'+set_type+'-src.txt', 'w')\n",
    "    tgt = open(path+\"/\"+tokenization+ '-'+set_type+'-tgt.txt', 'w')\n",
    "    \n",
    "    for i in source_data:\n",
    "        src.write(i+\"\\n\")\n",
    "    src.close() \n",
    "    for i in target_data:\n",
    "        tgt.write(i+\"\\n\")\n",
    "    tgt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Tokenization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNu Tokenization\n",
    "\n",
    "Preprocess the sentences so that only letters and numbers are left "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nahuatl</th>\n",
       "      <th>Spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amanteca toyauan qn iyaoan in aquique in canin...</td>\n",
       "      <td>amanteca yoyauan quiere decir sus enemigos son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anomatia qn quitoz nequi nixpan in omito yauyu...</td>\n",
       "      <td>anomatia quiere decir no delante de mí se habl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca nican tica nopiltzé xolé ca o toyollo on pa...</td>\n",
       "      <td>aquí estás hijo mío chaval ha quedado satisfec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca nican tonca nopiltzé nopiltziné notelpuchtl...</td>\n",
       "      <td>aquí estás hijo mío hijito mío muchacho mío mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca nican tica tle ticmati</td>\n",
       "      <td>aquí estás qué es lo que piensas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Nahuatl  \\\n",
       "0  amanteca toyauan qn iyaoan in aquique in canin...   \n",
       "1  anomatia qn quitoz nequi nixpan in omito yauyu...   \n",
       "2  ca nican tica nopiltzé xolé ca o toyollo on pa...   \n",
       "3  ca nican tonca nopiltzé nopiltziné notelpuchtl...   \n",
       "4                          ca nican tica tle ticmati   \n",
       "\n",
       "                                             Spanish  \n",
       "0  amanteca yoyauan quiere decir sus enemigos son...  \n",
       "1  anomatia quiere decir no delante de mí se habl...  \n",
       "2  aquí estás hijo mío chaval ha quedado satisfec...  \n",
       "3  aquí estás hijo mío hijito mío muchacho mío mi...  \n",
       "4                   aquí estás qué es lo que piensas  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parallelize data\n",
    "nah = clean_dataset(df['Náhuatl'])\n",
    "spa = clean_dataset(df['Español'])\n",
    "parallel_data_lenu = pd.DataFrame([[i,j] for i,j in zip(nah,spa)], columns = ['Nahuatl','Spanish'])\n",
    "parallel_data_lenu.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split Train and Validation Set \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    parallel_data_lenu['Nahuatl'], \n",
    "    parallel_data_lenu['Spanish'], \n",
    "    test_size=0.2, \n",
    "    random_state=30)\n",
    "\n",
    "# Split between validation test and test set\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_val, \n",
    "    y_val, \n",
    "    test_size=0.5, \n",
    "    random_state=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 14320\n",
      "Size of development set: 1790\n",
      "Size of test set: 1791\n"
     ]
    }
   ],
   "source": [
    "print( 'Size of training set:', len(X_train))\n",
    "print( 'Size of development set:', len(X_dev))\n",
    "print( 'Size of test set:', len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traing\n",
    "write_file(source_data = X_train , \n",
    "           target_data = y_train , \n",
    "           tokenization = 'LeNu', \n",
    "           set_type = 'train', \n",
    "           path = os.getcwd()+'/data/LeNu')\n",
    "#dev\n",
    "write_file(source_data = X_dev , \n",
    "           target_data = y_dev , \n",
    "           tokenization = 'LeNu', \n",
    "           set_type = 'dev', \n",
    "           path = os.getcwd()+'/data/LeNu')\n",
    "\n",
    "#test\n",
    "write_file(source_data = X_test , \n",
    "           target_data = y_test , \n",
    "           tokenization = 'LeNu', \n",
    "           set_type = 'test', \n",
    "           path = os.getcwd()+'/data/LeNu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-only Tokenization to use in Morfessor\n",
    "\n",
    "Preprocess sentences so that only letters are left\n",
    "Will not be used for translation, used in Morfessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nahuatl</th>\n",
       "      <th>Spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amanteca toyauan qn iyaoan in aquique in canin...</td>\n",
       "      <td>amanteca yoyauan quiere decir sus enemigos son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anomatia qn quitoz nequi nixpan in omito yauyu...</td>\n",
       "      <td>anomatia quiere decir no delante de mí se habl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca nican tica nopiltzé xolé ca o toyollo on pa...</td>\n",
       "      <td>aquí estás hijo mío chaval ha quedado satisfec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca nican tonca nopiltzé nopiltziné notelpuchtl...</td>\n",
       "      <td>aquí estás hijo mío hijito mío muchacho mío mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca nican tica tle ticmati</td>\n",
       "      <td>aquí estás qué es lo que piensas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Nahuatl  \\\n",
       "0  amanteca toyauan qn iyaoan in aquique in canin...   \n",
       "1  anomatia qn quitoz nequi nixpan in omito yauyu...   \n",
       "2  ca nican tica nopiltzé xolé ca o toyollo on pa...   \n",
       "3  ca nican tonca nopiltzé nopiltziné notelpuchtl...   \n",
       "4                          ca nican tica tle ticmati   \n",
       "\n",
       "                                             Spanish  \n",
       "0  amanteca yoyauan quiere decir sus enemigos son...  \n",
       "1  anomatia quiere decir no delante de mí se habl...  \n",
       "2  aquí estás hijo mío chaval ha quedado satisfec...  \n",
       "3  aquí estás hijo mío hijito mío muchacho mío mi...  \n",
       "4                   aquí estás qué es lo que piensas  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parallelize data\n",
    "nah_charonly = clean_dataset(df['Náhuatl'], numeric=False)\n",
    "spa_charonly = clean_dataset(df['Español'], numeric=False)\n",
    "parallel_data_charonly = pd.DataFrame([[i,j] for i,j in zip(nah_charonly,spa_charonly)], columns = [ 'Nahuatl','Spanish'])\n",
    "parallel_data_charonly.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into Training, Development and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split Train and Validation Set \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    parallel_data_charonly['Nahuatl'], \n",
    "    parallel_data_charonly['Spanish'], \n",
    "    test_size=0.2, \n",
    "    random_state=30)\n",
    "\n",
    "# Split between validation test and test set\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_val, \n",
    "    y_val, \n",
    "    test_size=0.5, \n",
    "    random_state=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "write_file(source_data = X_train , \n",
    "           target_data = y_train , \n",
    "           tokenization = 'charonly', \n",
    "           set_type = 'train', \n",
    "           path = os.getcwd()+'/data/character_only')\n",
    "#dev\n",
    "write_file(source_data = X_dev , \n",
    "           target_data = y_dev , \n",
    "           tokenization = 'charonly', \n",
    "           set_type = 'dev', \n",
    "           path = os.getcwd()+'/data/character_only')\n",
    "\n",
    "#test\n",
    "write_file(source_data = X_test , \n",
    "           target_data = y_test , \n",
    "           tokenization = 'charonly', \n",
    "           set_type = 'test', \n",
    "           path = os.getcwd()+'/data/character_only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNuP Tokenization\n",
    "Filter dat so that only Characters, Numbers, and Punctuation Marks are left in the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nahuatl</th>\n",
       "      <th>Spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amanteca toyauan q . n .  iyaoan in aquique in...</td>\n",
       "      <td>' amanteca yoyauan quiere decir  ' sus enemig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>' anomatia q . n .   ( quitoz nequi )  nixpan...</td>\n",
       "      <td>' anomatia quiere decir  ' no delante de mí s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca nican tica nopiltzé xolé ca o toyollo on pa...</td>\n",
       "      <td>' aquí estás ,  hijo mío ,  chaval ha quedado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>' ca nican tonca nopiltzé ,  nopiltziné ,  no...</td>\n",
       "      <td>' aquí estás ,  hijo mío ,  hijito mío ,  muc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>' ca nican tica tle ticmati ?</td>\n",
       "      <td>' aquí estás ,   ¿ qué es lo que piensas ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Nahuatl  \\\n",
       "0  amanteca toyauan q . n .  iyaoan in aquique in...   \n",
       "1   ' anomatia q . n .   ( quitoz nequi )  nixpan...   \n",
       "2  ca nican tica nopiltzé xolé ca o toyollo on pa...   \n",
       "3   ' ca nican tonca nopiltzé ,  nopiltziné ,  no...   \n",
       "4                     ' ca nican tica tle ticmati ?    \n",
       "\n",
       "                                             Spanish  \n",
       "0   ' amanteca yoyauan quiere decir  ' sus enemig...  \n",
       "1   ' anomatia quiere decir  ' no delante de mí s...  \n",
       "2   ' aquí estás ,  hijo mío ,  chaval ha quedado...  \n",
       "3   ' aquí estás ,  hijo mío ,  hijito mío ,  muc...  \n",
       "4        ' aquí estás ,   ¿ qué es lo que piensas ?   "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parallelize data\n",
    "nah_lenup = further_clean_dataset(df['Náhuatl'])\n",
    "spa_lenup = further_clean_dataset(df['Español'])\n",
    "parallel_data_lenup = pd.DataFrame([[i,j] for i,j in zip(nah_lenup,spa_lenup)], columns = [ 'Nahuatl','Spanish'])\n",
    "parallel_data_lenup.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into Training, Development and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split Train and Validation Set \n",
    "X_train, X_val, y_train, y_val = train_test_split( \n",
    "    parallel_data_lenup['Nahuatl'], \n",
    "    parallel_data_lenup['Spanish'],                     \n",
    "    test_size=0.2, \n",
    "    random_state=30)\n",
    "\n",
    "# Split between validation test and test set\n",
    "X_dev, X_test, y_dev, y_test = train_test_split( \n",
    "    X_val, \n",
    "    y_val, \n",
    "    test_size=0.5, \n",
    "    random_state=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "write_file(source_data = X_train , \n",
    "           target_data = y_train , \n",
    "           tokenization = 'LeNuP', \n",
    "           set_type = 'train', \n",
    "           path = os.getcwd()+'/data/LeNuP')\n",
    "#dev\n",
    "write_file(source_data = X_dev , \n",
    "           target_data = y_dev , \n",
    "           tokenization = 'LeNuP', \n",
    "           set_type = 'dev', \n",
    "           path = os.getcwd()+'/data/LeNuP')\n",
    "\n",
    "#test\n",
    "write_file(source_data = X_test , \n",
    "           target_data = y_test , \n",
    "           tokenization = 'LeNuP', \n",
    "           set_type = 'test', \n",
    "           path = os.getcwd()+'/data/LeNuP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morfessor 2.0 Tokenization\n",
    "Morfessor is an unsupervised morphological Segmentation Library. In the next cells, we will transform the data using 3 different morfessor techniques, but first we will get the data into a format that Morfessor will be able to read. Sentences from Character-only tokenization will be used to perform morphological segmentations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import morfessor\n",
    "\n",
    "def get_for_morfessor(data):\n",
    "    final = []\n",
    "    for sentence in data:\n",
    "        n = [(1,i)for i in sentence.split()]\n",
    "        n.append((0,()))\n",
    "        final+=(n)\n",
    "    return final\n",
    "\n",
    "def transform_sentence(sentence, word_idx):\n",
    "    new_st = []\n",
    "    for i in sentence.split(): \n",
    "    \n",
    "        q =  i.lower().split()\n",
    "        if len(q) > 0  and q[0] in word_idx:\n",
    "            new_st+=(word_idx[q[0]])\n",
    "        else:\n",
    "            new_st.append(i) \n",
    "            \n",
    "    new_st = ' '.join(new_st)\n",
    "    return new_st\n",
    "        \n",
    "train_data = get_for_morfessor(nah_charonly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the tokenizer with only words\n",
    "Train the morfessor morphological segmentator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "............................................................\n",
      "............................................................\n",
      "............................................................\n",
      "............................................................\n",
      "............................................................\n",
      "............................................................\n",
      "............................................................"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 8s, sys: 3.31 s, total: 8min 11s\n",
      "Wall time: 8min 35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#create model, fill in parameters and train\n",
    "\n",
    "model_word = morfessor.BaselineModel()\n",
    "model_word.load_data(train_data, count_modifier=lambda x: 1)\n",
    "model_word.train_batch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model for use in later projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# create segmentations dictionary\n",
    "wordbased_segementations = dict((j,k) for i,j,k in model_word.get_segmentations())\n",
    "# Save Segmentations for later use\n",
    "with open('wordbased_segementations.pickle', 'wb') as handle:\n",
    "    pickle.dump(wordbased_segementations, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNu+MS\n",
    "### Morfessor, Letters, and Numbers \n",
    "\n",
    "\n",
    "Preprocess sentences by using transforming sentences using the morphological segmentation model created above and use LeNu tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfrom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nahuatl</th>\n",
       "      <th>Spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aman teca to ya uan q n iyao an in aquique in ...</td>\n",
       "      <td>amanteca yoyauan quiere decir sus enemigos son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a no matia q n qui toz nequi n ixpan in omito ...</td>\n",
       "      <td>anomatia quiere decir no delante de mí se habl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca nican tica nopil tzé xolé ca o to yollo on ...</td>\n",
       "      <td>aquí estás hijo mío chaval ha quedado satisfec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca nican tonca nopil tzé nopiltzin é no telpuc...</td>\n",
       "      <td>aquí estás hijo mío hijito mío muchacho mío mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca nican tica tle ticmati</td>\n",
       "      <td>aquí estás qué es lo que piensas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Nahuatl  \\\n",
       "0  aman teca to ya uan q n iyao an in aquique in ...   \n",
       "1  a no matia q n qui toz nequi n ixpan in omito ...   \n",
       "2  ca nican tica nopil tzé xolé ca o to yollo on ...   \n",
       "3  ca nican tonca nopil tzé nopiltzin é no telpuc...   \n",
       "4                          ca nican tica tle ticmati   \n",
       "\n",
       "                                             Spanish  \n",
       "0  amanteca yoyauan quiere decir sus enemigos son...  \n",
       "1  anomatia quiere decir no delante de mí se habl...  \n",
       "2  aquí estás hijo mío chaval ha quedado satisfec...  \n",
       "3  aquí estás hijo mío hijito mío muchacho mío mi...  \n",
       "4                   aquí estás qué es lo que piensas  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the data\n",
    "nah_LeNu_segmentations = [transform_sentence(sentence, word_idx=wordbased_segementations) for sentence in nah]\n",
    "parallel_LeNu_segmentations = pd.DataFrame([[i,j] for i,j in zip(nah_LeNu_segmentations,spa)], columns = [ 'Nahuatl','Spanish'])\n",
    "parallel_LeNu_segmentations.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into Training, Development and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split Train and Validation Set \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    parallel_LeNu_segmentations['Nahuatl'], \n",
    "    parallel_LeNu_segmentations['Spanish'], \n",
    "    test_size=0.2, \n",
    "    random_state=30)\n",
    "\n",
    "# Split between validation test and test set\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_val, \n",
    "    y_val, \n",
    "    test_size=0.5, \n",
    "    random_state=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "write_file(source_data = X_train , \n",
    "           target_data = y_train , \n",
    "           tokenization = 'LeNu+MS', \n",
    "           set_type = 'train', \n",
    "           path = os.getcwd()+'/data/LeNu+MS')\n",
    "#dev\n",
    "write_file(source_data = X_dev , \n",
    "           target_data = y_dev , \n",
    "           tokenization = 'LeNu+MS', \n",
    "           set_type = 'dev', \n",
    "           path = os.getcwd()+'/data/LeNu+MS')\n",
    "\n",
    "#test\n",
    "write_file(source_data = X_test , \n",
    "           target_data = y_test , \n",
    "           tokenization = 'LeNu+MS', \n",
    "           set_type = 'test', \n",
    "           path = os.getcwd()+'/data/LeNu+MS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNuP+MS\n",
    "### Morfessor, Characters, Numbers, Punctuation\n",
    "\n",
    "\n",
    "Preprocess sentences by using transforming sentences using the morphological segmentation model created above and use LeNuP tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfrom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nahuatl</th>\n",
       "      <th>Spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aman teca to ya uan q . n . iyao an in aquique...</td>\n",
       "      <td>' amanteca yoyauan quiere decir  ' sus enemig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>' a no matia q . n . ( qui toz nequi ) n ixpan...</td>\n",
       "      <td>' anomatia quiere decir  ' no delante de mí s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca nican tica nopil tzé xolé ca o to yollo on ...</td>\n",
       "      <td>' aquí estás ,  hijo mío ,  chaval ha quedado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>' ca nican tonca nopil tzé , nopiltzin é , no ...</td>\n",
       "      <td>' aquí estás ,  hijo mío ,  hijito mío ,  muc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>' ca nican tica tle ticmati ?</td>\n",
       "      <td>' aquí estás ,   ¿ qué es lo que piensas ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Nahuatl  \\\n",
       "0  aman teca to ya uan q . n . iyao an in aquique...   \n",
       "1  ' a no matia q . n . ( qui toz nequi ) n ixpan...   \n",
       "2  ca nican tica nopil tzé xolé ca o to yollo on ...   \n",
       "3  ' ca nican tonca nopil tzé , nopiltzin é , no ...   \n",
       "4                      ' ca nican tica tle ticmati ?   \n",
       "\n",
       "                                             Spanish  \n",
       "0   ' amanteca yoyauan quiere decir  ' sus enemig...  \n",
       "1   ' anomatia quiere decir  ' no delante de mí s...  \n",
       "2   ' aquí estás ,  hijo mío ,  chaval ha quedado...  \n",
       "3   ' aquí estás ,  hijo mío ,  hijito mío ,  muc...  \n",
       "4        ' aquí estás ,   ¿ qué es lo que piensas ?   "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the data\n",
    "nah_LeNuP_segmentations = [transform_sentence(sentence, word_idx=wordbased_segementations) for sentence in nah_cnm]\n",
    "parallel_LeNuP_segmentations = pd.DataFrame([[i,j] for i,j in zip(nah_LeNuP_segmentations,spa_cnm)], columns = [ 'Nahuatl','Spanish'])\n",
    "parallel_LeNuP_segmentations.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into Training, Development and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split Train and Validation Set \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    parallel_LeNuP_segmentations['Nahuatl'], \n",
    "    parallel_LeNuP_segmentations['Spanish'], \n",
    "    test_size=0.2, \n",
    "    random_state=30)\n",
    "\n",
    "# Split between validation test and test set\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_val, \n",
    "    y_val, \n",
    "    test_size=0.5, \n",
    "    random_state=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "write_file(source_data = X_train , \n",
    "           target_data = y_train , \n",
    "           tokenization = 'LeNuP+MS', \n",
    "           set_type = 'train', \n",
    "           path = os.getcwd()+'/data/LeNuP+MS')\n",
    "#dev\n",
    "write_file(source_data = X_dev , \n",
    "           target_data = y_dev , \n",
    "           tokenization = 'LeNuP+MS', \n",
    "           set_type = 'dev', \n",
    "           path = os.getcwd()+'/data/LeNuP+MS')\n",
    "\n",
    "#test\n",
    "write_file(source_data = X_test , \n",
    "           target_data = y_test , \n",
    "           tokenization = 'LeNuP+MS', \n",
    "           set_type = 'test', \n",
    "           path = os.getcwd()+'/data/LeNuP+MS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNu + MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nahuatl</th>\n",
       "      <th>Spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aman teca to ya uan q n iyao an in aquique in ...</td>\n",
       "      <td>amanteca yoyauan quiere decir sus enemigos son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a no matia q n qui toz nequi n ixpan in omito ...</td>\n",
       "      <td>anomatia quiere decir no delante de mí se habl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca nican tica nopil tzé xolé ca o to yollo on ...</td>\n",
       "      <td>aquí estás hijo mío chaval ha quedado satisfec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca nican tonca nopil tzé nopiltzin é no telpuc...</td>\n",
       "      <td>aquí estás hijo mío hijito mío muchacho mío mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca nican tica tle ticmati</td>\n",
       "      <td>aquí estás qué es lo que piensas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Nahuatl  \\\n",
       "0  aman teca to ya uan q n iyao an in aquique in ...   \n",
       "1  a no matia q n qui toz nequi n ixpan in omito ...   \n",
       "2  ca nican tica nopil tzé xolé ca o to yollo on ...   \n",
       "3  ca nican tonca nopil tzé nopiltzin é no telpuc...   \n",
       "4                          ca nican tica tle ticmati   \n",
       "\n",
       "                                             Spanish  \n",
       "0  amanteca yoyauan quiere decir sus enemigos son...  \n",
       "1  anomatia quiere decir no delante de mí se habl...  \n",
       "2  aquí estás hijo mío chaval ha quedado satisfec...  \n",
       "3  aquí estás hijo mío hijito mío muchacho mío mi...  \n",
       "4                   aquí estás qué es lo que piensas  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the data\n",
    "nah_LeNu_MS = [transform_sentence(sentence, word_idx=wordbased_segementations) for sentence in nah]\n",
    "parallel_LeNu_MS = pd.DataFrame([[i,j] for i,j in zip(nah_LeNu_MS,spa)], columns = [ 'Nahuatl','Spanish'])\n",
    "parallel_LeNu_MS.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split Train and Validation Set \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    parallel_mcnm_segmentations['Nahuatl'], \n",
    "    parallel_mcnm_segmentations['Spanish'], \n",
    "    test_size=0.2, \n",
    "    random_state=30)\n",
    "\n",
    "# Split between validation test and test set\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_val, \n",
    "    y_val, \n",
    "    test_size=0.5, \n",
    "    random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "write_file(source_data = X_train , \n",
    "           target_data = y_train , \n",
    "           tokenization = 'morf_cnm', \n",
    "           set_type = 'train', \n",
    "           path = os.getcwd()+'/data/morf_cnm')\n",
    "#dev\n",
    "write_file(source_data = X_dev , \n",
    "           target_data = y_dev , \n",
    "           tokenization = 'morf_cnm', \n",
    "           set_type = 'dev', \n",
    "           path = os.getcwd()+'/data/morf_cnm')\n",
    "\n",
    "#test\n",
    "write_file(source_data = X_test , \n",
    "           target_data = y_test , \n",
    "           tokenization = 'morf_cnm', \n",
    "           set_type = 'test', \n",
    "           path = os.getcwd()+'/data/morf_cnm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
